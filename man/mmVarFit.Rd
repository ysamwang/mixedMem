% Generated by roxygen2 (4.0.2): do not edit by hand
\name{mmVarFit}
\alias{mmVarFit}
\title{Fit  Mixed Membership models using variational EM}
\usage{
mmVarFit(model)
}
\arguments{
\item{model}{a \code{mixedMemModel} object created by the \code{mixedMemModel} constructor}
}
\value{
a \code{mixedMemModel} containing updated variational parameters and hyperparameters
}
\description{
Main function of the \code{mixedMem} package. Fits parameters \eqn{\phi} and \eqn{\delta} for the variational
 distribution of latent variables as well as psuedo-MLE estimates for
 the population hyperparameters \eqn{\alpha} and \eqn{\theta}. See documentation for
 \code{mixedMemModel} or the package vignette for a more detailed description of
 the variables/notation in a mixed membership model.
}
\details{
\code{mmVarFit} selects psuedo-MLE estimates for \eqn{alpha} and \eqn{theta} and approximates
the posterior distribution for the latent variables through a mean field variational approach.
Using Jensen's inequality, we derive the lower bound on the RHS (sometimes called the ELBO) for the log-likelihood
of the data.

\eqn{P(obs |\alpha, \theta) \ge E_Q{\log[p(X,Z, \Lambda)]} - E_Q{\log[Q(Z, \Lambda|\phi, \delta)]}}

where

\eqn{Q = \prod_i [Dirichlet(\lambda_i|\phi_i) \prod_j^J \prod_r^{R_j} \prod_n^{N_{i,j,r}}Multinomial(Z_{i,j,r,n}|\delta_{i,j,r,n})]}

It can be shown that maximizing the ELBO with respect to \eqn{\phi} and \eqn{\delta}
minimizes the KL divergence, between a tractable variational distribution and the true posterior.
We can simultaneously pick psuedo-MLE hyperparameters \eqn{\alpha} and \eqn{\theta} to maximize the lower bound on
the log-likelihood of the observed data.

The method uses an EM approach. The E step considers the hyperparameters fixed and picks appropriate variational
parameters to minimize the KL divergence. On the M step
the variational parameters are fixed and hyperparmaters are selected which maximize the lower bound
on the log-likelihood.
}
\examples{
## Generate Data
Total <- 30 #30 Individuals
J <- 2 # 2 variables
dist <- rep("multinomial",J) #both variables are multinomial
Rj <- rep(100,J) #100 repititions for each variable
#Nijr will always be 1 for multinomials and bernoulli's
Nijr <- array(1, dim = c(Total, J, max(Rj)))
K <- 4 # 4 sub-populations
alpha <- rep(.5, K) #hyperparameter for dirichlet distribution
Vj <- rep(5, J) #each multinomial has 5 options
theta <- array(0, dim = c(J, K, max(Vj)))
theta[1,,] <- gtools::rdirichlet(K, rep(.3, 5))
theta[2,,] <- gtools::rdirichlet(K, rep(.3, 5))
lambda <- gtools::rdirichlet(Total, rep(.6,K))
obs = array(0, dim = c(Total, J, max(Rj), max(Nijr)))
for(i in 1:Total)
{
 for(j in 1:J)
 {
   for(r in 1:Rj[j])
   {
     for(n in Nijr[i,j,r])
     {
     # sub-population which governs the multinomial
     sub.pop <- sample.int(K, size = 1, prob = lambda[i,])
     #Note that observations must be from 0:(Vj-1)
     obs[i,j,r,n] <- sample.int(Vj[j], size = 1,prob = theta[j,sub.pop,])-1       }
   }
 }
}

## Initialize a mixedMemModel object
test_model <- mixedMemModel(Total = Total, J = J,Rj = Rj, Nijr= Nijr,
 K = K, Vj = Vj,dist = dist, obs = obs,
  alpha = alpha, theta = theta+0)

## Fit the mixed membership model
out <-mmVarFit(test_model)
}
\references{
Beal, Matthew James. Variational algorithms for approximate Bayesian inference. Diss. University of London, 2003.
}
\seealso{
mixedMemModel
}

